Project File Structure:
./
    .gitignore
    app.py
    config.py
    models.py
    project_structure.txt
    __init__.py
    instance/
        recipes.db
    recipe_scraper/
        import_recipes.py
        allerhande/
            allerhande.py
            recipes.csv
    static/
        dynamic_fields.js
        style.css
    templates/
        404.html
        index.html
        recipe_page.html

Contents of ['.py'] Files:

File: .\app.py

from flask import render_template, request, jsonify
from config import app, db
from models import Recipe, Ingredient


@app.route('/', methods=['GET', 'POST'])
def index():
    query = request.args.get('q', '')
    recipes = []
    if query:
        # Check if the query matches an ingredient
        matching_ingredient = Ingredient.query.filter(Ingredient.name.contains(query)).first()
        if matching_ingredient:
            # If it's an ingredient, find all recipes containing it
            recipes = Recipe.query.join(Recipe.ingredients).filter(Ingredient.name.contains(query)).all()
        else:
            # Otherwise, search for recipes by name
            recipes = Recipe.query.filter(Recipe.name.contains(query)).all()

    return render_template('index.html', recipes=recipes, query=query)


@app.route('/search', methods=['GET'])
def search():
    query = request.args.get('q', '')
    if query:
        ingredient_list = Ingredient.query.with_entities(Ingredient.name).filter(
            Ingredient.name.contains(query)).distinct().all()
        recipe_list = Recipe.query.with_entities(Recipe.name).filter(Recipe.name.contains(query)).distinct().all()
        suggestions = [ingredient.name for ingredient in ingredient_list] + [recipe.name for recipe in recipe_list]
        return jsonify(suggestions)
    return jsonify([])

@app.route('/recipe-page', methods=['GET'])
def recipe_page():
    recipe_id = request.args.get('recipe', '')
    if recipe_id:
        recipe = Recipe.query.get(recipe_id)
        if recipe:
            return render_template('recipe_page.html', recipe=recipe)
    return render_template('404.html')  # Template for page not found


if __name__ == '__main__':
    app.run(debug=True)



File: .\config.py

from flask import Flask
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///recipes.db'
db = SQLAlchemy(app)


File: .\models.py

from config import db

class Recipe(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    link = db.Column(db.String(255))
    time_prepare = db.Column(db.String(50))
    time_wait = db.Column(db.String(50))
    rating = db.Column(db.Float)
    tags = db.Column(db.String(255))
    servings = db.Column(db.String(50))
    ingredients = db.relationship('Ingredient', backref='recipe', lazy=True)
    steps = db.relationship('RecipeStep', backref='recipe', lazy=True)
    equipment = db.Column(db.String(255))
    image = db.Column(db.String(255))

class Ingredient(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    amount = db.Column(db.String(50))
    recipe_id = db.Column(db.Integer, db.ForeignKey('recipe.id'), nullable=False)

class RecipeStep(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    step = db.Column(db.Text, nullable=False)
    recipe_id = db.Column(db.Integer, db.ForeignKey('recipe.id'), nullable=False)
    order = db.Column(db.Integer)  # To maintain the order of steps


File: .\__init__.py

import os
import shutil
from app import app
from models import db

def delete_instance_folder(path='instance'):
    if os.path.exists(path):
        shutil.rmtree(path)

def create_directory_for_db(path='instance'):
    if not os.path.exists(path):
        os.makedirs(path)

with app.app_context():
    # Delete the instance folder
    delete_instance_folder()

    # Create the necessary directory for the SQLite database
    create_directory_for_db()
    # Recreate the database
    db.create_all()
    # Import and run the import_recipes script
    from recipe_scraper.import_recipes import main as import_recipes_main
    import_recipes_main()



File: .\recipe_scraper\import_recipes.py

import csv
import ast  # To safely evaluate strings as Python literals
from models import db, Recipe, Ingredient, RecipeStep
from app import app

def import_csv(csv_file):
    with open(csv_file, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        for row in reader:
            recipe = Recipe(
                name=row['name'],
                link=row['link'],
                time_prepare=row['time_prepare'],
                time_wait=row['time_wait'],
                rating=float(row['rating']) if row['rating'] else None,
                tags=row['tags'],
                servings=row['servings'],
                equipment=row['equipment'],
                image=row['image']
            )
            db.session.add(recipe)
            db.session.flush()

            # Parse ingredients
            ingredients_list = ast.literal_eval(row['ingredients'])
            for ing in ingredients_list:
                ingredient = Ingredient(name=ing[1], amount=ing[0], recipe_id=recipe.id)
                db.session.add(ingredient)

            # Parse steps
            steps_list = ast.literal_eval(row['steps'])
            for order, step in enumerate(steps_list):
                recipe_step = RecipeStep(step=step, recipe_id=recipe.id, order=order)
                db.session.add(recipe_step)

            db.session.commit()

def main():
    with app.app_context():
        import_csv('recipe_scraper/allerhande/recipes.csv')

# The following lines should be at the very bottom of the file
if __name__ == "__main__":
    main()


File: .\recipe_scraper\allerhande\allerhande.py

from concurrent.futures import ThreadPoolExecutor
import time
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import csv

# Initialize constants
MAX_THREADS = 5  # Max number of concurrent threads
REQUEST_INTERVAL = 4  # Seconds between requests in each thread

def init_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36')
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def scrape_recipe_links(base_url, num_pages):
    driver = init_driver()
    all_recipe_links = []

    for i in range(num_pages + 1):
        driver.get(f"{base_url}&page={i}" if i > 0 else base_url)
        time.sleep(5)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for div in soup.find_all('div', class_='column xxlarge-4 large-6 small-12'):
            link_tag = div.find('a', href=True)
            if link_tag and "recept/R-" in link_tag.get('href', ''):
                name = link_tag.get('aria-label', '').replace('Recept: ', '')
                link = "https://www.ah.nl" + link_tag['href']
                img_tag = div.find('img', class_='card-image-set_imageSet__Su7xI')
                image_src = img_tag['data-srcset'].split(', ')[-1].split(' ')[0] if img_tag else ''
                all_recipe_links.append({'name': name, 'link': link, 'image': image_src})
                print(f"{name}: link retrieved")

    driver.quit()
    return all_recipe_links

def scrape_recipe_details(recipe_links, scraped_recipes):
    driver = init_driver()
    all_recipes = []

    for recipe in recipe_links:
        if recipe['link'] in scraped_recipes:
            continue # Skip already scraped recipes

        # Rate limiting
        time.sleep(REQUEST_INTERVAL)

        driver.get(recipe['link'])
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Simplified scraping logic
        recipe_info = {
            'name': recipe['name'],
            'link': recipe['link'],
            'time_prepare': get_time_element(soup, 0),
            'time_wait': get_time_element(soup, 1),
            'rating': get_rating(soup),
            'tags': [tag.text for tag in soup.select('.recipe-header-tags_tags__DFsas li .recipe-tag_text__aKcWG')],
            'servings': get_text(soup, '.recipe-ingredients_count__zS2P-'),
            'ingredients': [(amount.text.strip(), name.text.strip()) for amount, name in zip(soup.select('.recipe-ingredients_ingredientsList__thXVo .ingredient_unit__-ptEq'), soup.select('.recipe-ingredients_ingredientsList__thXVo .ingredient_name__WXu5R'))],
            'equipment': [item.text.strip() for item in soup.select('.recipe-ingredients_kitchen__Ag6XI p.typography_root__Om3Wh')],
            'steps': [step.text.strip() for step in soup.select('.recipe-steps_step__FYhB8 p.typography_root__Om3Wh')],
            'image': recipe['image'],
        }
        print(recipe_info)
        all_recipes.append(recipe_info)

    driver.quit()
    return all_recipes

def get_time_element(soup, index):
    elements = soup.select('.recipe-header-time_timeLine__nn84w')
    return elements[index].text if len(elements) > index else None

def get_rating(soup):
    star_full_count = len(soup.select('.allerhande-icon.svg.svg--svg_star'))
    star_half_count = len(soup.select('.allerhande-icon.svg.svg--svg_star-half'))
    return star_full_count + star_half_count / 2

def get_text(soup, selector):
    element = soup.select_one(selector)
    return element.text if element else None

def save_to_csv(detailed_recipes, filename='recipes.csv'):
    # Append to existing CSV instead of overwriting
    with open(filename, 'a', newline='', encoding='utf-8') as output_file:
        dict_writer = csv.DictWriter(output_file, detailed_recipes[0].keys())
        dict_writer.writerows(detailed_recipes)

def read_existing_recipes(filename='recipes.csv'):
    try:
        with open(filename, newline='', encoding='utf-8') as file:
            return {row['link'] for row in csv.DictReader(file)}
    except FileNotFoundError:
        return set()

def main():
    base_url = "https://www.ah.nl/allerhande/recepten-zoeken?menugang=hoofdgerecht"
    num_pages = 3

    scraped_recipes = read_existing_recipes()
    recipe_links = scrape_recipe_links(base_url, num_pages)

    # Filter links to include only those that haven't been scraped yet
    new_recipe_links = [link for link in recipe_links if link['link'] not in scraped_recipes]

    # Using ThreadPoolExecutor for multithreading
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # Submit each batch of new recipe links to a separate thread
        futures = [executor.submit(scrape_recipe_details, new_recipe_links[i:i + MAX_THREADS], scraped_recipes)
                   for i in range(0, len(new_recipe_links), MAX_THREADS)]

        # Retrieve results from futures
        detailed_recipes = []
        for future in futures:
            detailed_recipes.extend(future.result())

    save_to_csv(detailed_recipes)

if __name__ == "__main__":
    main()

